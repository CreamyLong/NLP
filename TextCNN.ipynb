{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"I\",\"like\",\"this\",\"movie\",\"very\",\"much\",\"!\",\"nlp\",\"is\",\"great\"]\n",
    "embed_size = 5\n",
    "vocab_size = 11 # 10个词，第一个位置留给补零的位置\n",
    "class_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) #词向量，可用word2vec\n",
    "        self.conv4 = nn.Conv1d(in_channels=embed_size, out_channels=2, kernel_size=4, stride = 1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=embed_size, out_channels=2, kernel_size=3, stride = 1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=embed_size, out_channels=2, kernel_size=2, stride = 1)\n",
    "        \n",
    "        hidden_size = 2*3\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_size, class_num)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # [batch_size, seq_length]->[batch_size, seq_length,seq_embed_size]\n",
    "#         print(x.size()) torch.Size([2, 6, 5])\n",
    "        x = x.transpose(2,1) # [batch_size, seq_embed_size, seq_length]\n",
    "#         print(x.size()) torch.Size([2, 5, 6]\n",
    "        x1 = self.conv2(x) # [batch_size, out_channels, seq_length-k+l]\n",
    "        \n",
    "        x1 = F.relu(x1) # [batch_size, out_channels, seq_length-k+l]\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)) # [batch_size, out_channels]\n",
    "        \n",
    "        x2 = self.conv3(x)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2))\n",
    "        \n",
    "        x3 = self.conv4(x)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = F.max_pool1d(x3, x3.size(2))\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 1) #[batch_size, out_channels*3 , 1]\n",
    "        x = self.dropout(x)\n",
    "        x = x.squeeze() #[batch_size, out_channels*3]\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [[\"I\",\"like\",\"this\",\"movie\",\"very\",\"much\",\"!\"],\n",
    "            ['nlp',\"is\",\"great\"]]\n",
    "label = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2word = {}\n",
    "for index,word in enumerate(vocab):\n",
    "    word2id[word] = index+1\n",
    "    id2word[index+1]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7], [8, 9, 10]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids = [[word2id[w] for w in s] for s in train_text]\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "短的补0，过长的截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6], [8, 9, 10, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 6\n",
    "for i in range(len(train_ids)):\n",
    "    s = train_ids[i]\n",
    "    train_ids[i] = s[:max_length]+[0]*(max_length-len(s))\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor = torch.tensor(train_ids)\n",
    "train_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.tensor(label)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embed): Embedding(11, 5)\n",
       "  (conv4): Conv1d(5, 2, kernel_size=(4,), stride=(1,))\n",
       "  (conv3): Conv1d(5, 2, kernel_size=(3,), stride=(1,))\n",
       "  (conv2): Conv1d(5, 2, kernel_size=(2,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (linear): Linear(in_features=6, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6908247470855713\n",
      "0.9275128841400146\n",
      "0.8639361262321472\n",
      "0.7146687507629395\n",
      "0.5801581144332886\n",
      "0.8843567967414856\n",
      "0.8364002704620361\n",
      "0.7429993152618408\n",
      "0.6341226696968079\n",
      "0.49596062302589417\n",
      "0.8533338308334351\n",
      "0.676983118057251\n",
      "0.8743958473205566\n",
      "0.4713447391986847\n",
      "0.6170797348022461\n",
      "0.7254218459129333\n",
      "0.6794955730438232\n",
      "0.8573406934738159\n",
      "0.7313684225082397\n",
      "0.6111850738525391\n",
      "0.6743123531341553\n",
      "0.699020266532898\n",
      "0.518700897693634\n",
      "0.6605005264282227\n",
      "0.6464823484420776\n",
      "0.7326514720916748\n",
      "0.6220919489860535\n",
      "0.49077868461608887\n",
      "0.8059700131416321\n",
      "0.3801306486129761\n",
      "0.45203810930252075\n",
      "0.4112955927848816\n",
      "0.8763601183891296\n",
      "0.3816373348236084\n",
      "1.004064679145813\n",
      "0.46701598167419434\n",
      "0.6195912957191467\n",
      "0.4708707332611084\n",
      "0.5191164612770081\n",
      "0.956315815448761\n",
      "0.3962726593017578\n",
      "0.48859870433807373\n",
      "0.4357954263687134\n",
      "0.6388231515884399\n",
      "0.8236682415008545\n",
      "0.7422412037849426\n",
      "0.9560737013816833\n",
      "0.6414955854415894\n",
      "0.7805585861206055\n",
      "0.8167399764060974\n",
      "0.6091652512550354\n",
      "0.7299103736877441\n",
      "0.6220731139183044\n",
      "0.5334717035293579\n",
      "0.6937392950057983\n",
      "0.7805575132369995\n",
      "0.6889734268188477\n",
      "0.570198118686676\n",
      "0.8447049856185913\n",
      "0.9515854716300964\n",
      "0.5346041321754456\n",
      "0.5148122310638428\n",
      "0.6550564765930176\n",
      "0.34328535199165344\n",
      "0.6295843124389648\n",
      "0.7315897345542908\n",
      "0.36218297481536865\n",
      "0.36064261198043823\n",
      "0.7133011817932129\n",
      "0.31249314546585083\n",
      "0.34915295243263245\n",
      "0.6289872527122498\n",
      "0.5587443113327026\n",
      "0.3884221315383911\n",
      "0.42560189962387085\n",
      "0.3515912592411041\n",
      "0.750196099281311\n",
      "0.6799935102462769\n",
      "0.5035223364830017\n",
      "0.9112087488174438\n",
      "0.4973425567150116\n",
      "0.6987894177436829\n",
      "0.3910742998123169\n",
      "0.5422642230987549\n",
      "0.2785945236682892\n",
      "0.3501642048358917\n",
      "0.6682968735694885\n",
      "0.6094796061515808\n",
      "0.3813926577568054\n",
      "0.5464012026786804\n",
      "0.3659597635269165\n",
      "0.5753129124641418\n",
      "0.31814560294151306\n",
      "0.26046812534332275\n",
      "0.5159947872161865\n",
      "0.6701529026031494\n",
      "0.4458450675010681\n",
      "0.49030232429504395\n",
      "0.48711520433425903\n",
      "0.8237994909286499\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model.zero_grad()\n",
    "    outputs = model(train_tensor)\n",
    "    loss = F.cross_entropy(outputs, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
